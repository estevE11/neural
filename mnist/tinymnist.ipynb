{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist dataset\n",
    "\n",
    "from dataset import fetch_mnist\n",
    "X_train, Y_train, X_test, Y_test = fetch_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn\n",
    "class TinyMNIST:\n",
    "    def __init__(self):\n",
    "        self.l1 = nn.Linear(784, 128, bias=True)\n",
    "        self.l2 = nn.Linear(128, 10, bias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.l1(x)\n",
    "        x = x.relu()\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gpuctypes.cuda' has no attribute 'nvrtcCreateProgram'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msparse_categorical_crossentropy(target)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/nn/optim.py:37\u001b[0m, in \u001b[0;36mSGD.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# this is needed since the grads can form a \"diamond\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# TODO: fix this in lazy.py\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m g \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwd \u001b[38;5;241m*\u001b[39m t\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum:\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/tensor.py:101\u001b[0m, in \u001b[0;36mTensor.realize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrealize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m   \u001b[43mrun_schedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazydata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/realize.py:28\u001b[0m, in \u001b[0;36mrun_schedule\u001b[0;34m(schedule)\u001b[0m\n\u001b[1;32m     25\u001b[0m si \u001b[38;5;241m=\u001b[39m schedule\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# get the program\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m prg \u001b[38;5;241m=\u001b[39m \u001b[43mlower_schedule_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# invalidate the output buffer if there's a non contig usage of it in inputs\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m si\u001b[38;5;241m.\u001b[39mout\u001b[38;5;241m.\u001b[39moutput_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/realize.py:21\u001b[0m, in \u001b[0;36mlower_schedule_item\u001b[0;34m(si)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01mis\u001b[39;00m LoadOps\u001b[38;5;241m.\u001b[39mCOPY: \u001b[38;5;28;01mreturn\u001b[39;00m BufferCopy\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39mop \u001b[38;5;129;01mis\u001b[39;00m LoadOps\u001b[38;5;241m.\u001b[39mCUSTOM: \u001b[38;5;28;01mreturn\u001b[39;00m CustomOp(si\u001b[38;5;241m.\u001b[39mast\u001b[38;5;241m.\u001b[39marg)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDevice\u001b[49m\u001b[43m[\u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43msi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/device.py:314\u001b[0m, in \u001b[0;36mCompiled.get_runner\u001b[0;34m(self, ast)\u001b[0m\n\u001b[1;32m    313\u001b[0m functools\u001b[38;5;241m.\u001b[39mlru_cache(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# pylint: disable=method-cache-max-size-none\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_runner\u001b[39m(\u001b[38;5;28mself\u001b[39m, ast:LazyOp) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledASTRunner: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_program(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_linearizer(ast))\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/device.py:286\u001b[0m, in \u001b[0;36mCompiled.to_program\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    284\u001b[0m k\u001b[38;5;241m.\u001b[39mlinearize()\n\u001b[1;32m    285\u001b[0m src, runtime_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer(to_function_name(k\u001b[38;5;241m.\u001b[39mname), k\u001b[38;5;241m.\u001b[39muops)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCompiledASTRunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mruntime\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/device.py:254\u001b[0m, in \u001b[0;36mCompiledASTRunner.build\u001b[0;34m(self, compiler, runtime)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler, runtime):\n\u001b[0;32m--> 254\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib \u001b[38;5;241m=\u001b[39m compiler\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprg) \u001b[38;5;28;01mif\u001b[39;00m getenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDISABLE_COMPILER_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclprg \u001b[38;5;241m=\u001b[39m runtime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib)\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/helpers.py:260\u001b[0m, in \u001b[0;36mdiskcache.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m table, key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, hashlib\u001b[38;5;241m.\u001b[39msha256(pickle\u001b[38;5;241m.\u001b[39mdumps((args, kwargs)))\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (ret\u001b[38;5;241m:=\u001b[39mdiskcache_get(table, key)): \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diskcache_put(table, key, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/dev/tinygrad/tinygrad/runtime/ops_cuda.py:23\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[0;34m(prg)\u001b[0m\n\u001b[1;32m     22\u001b[0m diskcache\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile_cuda\u001b[39m(prg) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m compile_cuda_style(prg, [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--gpu-architecture=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCUDADevice\u001b[38;5;241m.\u001b[39mdefault_arch_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I/usr/local/cuda/include\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-I/usr/include\u001b[39m\u001b[38;5;124m\"\u001b[39m], cuda\u001b[38;5;241m.\u001b[39mnvrtcProgram, \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnvrtcCreateProgram\u001b[49m, cuda\u001b[38;5;241m.\u001b[39mnvrtcCompileProgram, cuda\u001b[38;5;241m.\u001b[39mnvrtcGetPTX, cuda\u001b[38;5;241m.\u001b[39mnvrtcGetPTXSize, cuda\u001b[38;5;241m.\u001b[39mnvrtcGetProgramLog, cuda\u001b[38;5;241m.\u001b[39mnvrtcGetProgramLogSize, check)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gpuctypes.cuda' has no attribute 'nvrtcCreateProgram'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GPU'] = '1'\n",
    "\n",
    "model = TinyMNIST()\n",
    "optim = nn.optim.SGD([model.l1.weight, model.l2.weight], lr=0.001)\n",
    "\n",
    "BS = 32\n",
    "with Tensor.train():\n",
    "    for i in range(400):\n",
    "        samp = np.random.randint(0, X_train.shape[0], size=(BS, 1))\n",
    "        ipt = Tensor(X_train[samp]).float()\n",
    "        samp = samp.reshape(BS)\n",
    "        target = Tensor(Y_train[samp]).float()\n",
    "        optim.zero_grad()\n",
    "        out = model(ipt)\n",
    "        loss = out.sparse_categorical_crossentropy(target)\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15935.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 91.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "samples = 1000\n",
    "for i in range(samples):\n",
    "    samp = np.random.randint(0, X_test.shape[0], size=(1, 1))\n",
    "    X = Tensor(X_test[samp]).float()\n",
    "    Y = Y_test[samp][0][0]\n",
    "    guess = model(X).argmax()\n",
    "    if Y == guess.item(): corr += 1\n",
    "print(f\"accuracy: {corr/samples*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"models/mnist_1.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
